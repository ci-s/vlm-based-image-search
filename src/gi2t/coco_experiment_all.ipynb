{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from pycocotools.coco import COCO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from eval.metrics import Metrics\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from services.settings import settings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/prakt/s0077/miniconda3/envs/image-search/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "k = 10\n",
    "threshold = 0.3\n",
    "last_index = 500\n",
    "\n",
    "options_with_all = [\"avg_embed\", \"highest_singular_score\",\"avg_score\"]\n",
    "\n",
    "all_queries_path = os.path.join(settings.project_root_dir,\"src/eval/extended_queries.json\" )\n",
    "all_queries = json.load(open(all_queries_path))\n",
    "response_dict_path = os.path.join(settings.output_dir, \"response_dict_git_500.json\")\n",
    "\n",
    "encoder_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "url_prefix=\"http://images.cocodataset.org/val2017/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def set_coco_object(data_dir):\n",
    "    ann_file = os.path.join(data_dir, 'annotations/captions_val2017.json')\n",
    "    coco = COCO(ann_file)\n",
    "    return coco\n",
    "\n",
    "def embed_text(text,embedder):\n",
    "    return embedder.encode([text])\n",
    "\n",
    "def get_image_urls(img_ids, coco):\n",
    "    img_urls = []\n",
    "    for img_id in img_ids:\n",
    "        img = coco.loadImgs(img_id)[0]\n",
    "        img_urls.append(img['coco_url'])\n",
    "    return img_urls\n",
    "\n",
    "def get_coco_image_captions(coco, image_id):\n",
    "    annIds = coco.getAnnIds(imgIds=image_id)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    captions = [ann['caption'] for ann in anns]\n",
    "    return captions\n",
    "\n",
    "def fetch_image(url):\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    return image\n",
    "\n",
    "def show_images(img_urls):\n",
    "    fig, axs = plt.subplots(1, len(img_urls), figsize=(20, 20))\n",
    "    for i, url in enumerate(img_urls):\n",
    "        img = fetch_image(url)\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "coco = set_coco_object(os.path.join(settings.data_dir, 'coco-images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric related functions\n",
    "def get_scores(retrieved_files, gt_retrieved_files, performance_dict):\n",
    "    performance_dict[\"precision\"].append(Metrics.precision(retrieved_files, gt_retrieved_files))\n",
    "    performance_dict[\"recall\"].append(Metrics.recall(retrieved_files, gt_retrieved_files))\n",
    "    performance_dict[\"f1\"].append(Metrics.f1_score(retrieved_files, gt_retrieved_files))\n",
    "    performance_dict[\"nDCG\"].append(Metrics.ndcg(retrieved_files, gt_retrieved_files, 5))\n",
    "    return performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, K, SIZE,performance_dict, avg_embed_sim=False, highest_singular_sim=False, highest_avg_sim=False):\n",
    "    query_embedding = embed_text(query, embedder)\n",
    "    # this loads from a file i saved while creating embeddings\n",
    "    # but taking the first 500 id from coco object would be the same\n",
    "    # or i can also share the ids for the images\n",
    "    img_ids = [int(line.strip()) for line in open('../../data/coco_embeddings/coco_image_ids_' + str(SIZE) +'.txt')]\n",
    "    \n",
    "    # load GIT embeddings and calculate similarities with the query embedding, then get top K images\n",
    "    git_embeddings = np.load('../../data/git_embeddings/git_caption_embeddings_' + str(SIZE) +'.npy')\n",
    "    git_similarity_scores = cosine_similarity(query_embedding, git_embeddings).flatten()\n",
    "    git_topk = np.argsort(git_similarity_scores)[::-1][:K]\n",
    "    \n",
    "    # 1st option: needs averaged embeddings\n",
    "    if avg_embed_sim:\n",
    "        ground_truth_embeddings = np.load('../../data/coco_embeddings/coco_embeddings_' + str(SIZE) +'_averaged.npy')\n",
    "        ground_truth_similarity_scores= cosine_similarity(query_embedding, ground_truth_embeddings).flatten()\n",
    "    # other 2 options: needs individual embeddings    \n",
    "    else:\n",
    "        ground_truth_embeddings = np.load('../../data/coco_embeddings/coco_embeddings_' + str(SIZE) +'.npy') # (SIZE, 5, 384)\n",
    "        ground_truth_similarity_scores = []\n",
    "        for caption_embeddings_per_image in ground_truth_embeddings:\n",
    "            sim_scores = cosine_similarity(query_embedding, caption_embeddings_per_image).flatten()\n",
    "            if highest_singular_sim:\n",
    "                # get the highest similarity score per image\n",
    "                singular_sim = np.max(sim_scores)\n",
    "                ground_truth_similarity_scores.append(singular_sim)\n",
    "            elif highest_avg_sim:\n",
    "                # get the average similarity score per image\n",
    "                avg_sim = np.mean(sim_scores)\n",
    "                ground_truth_similarity_scores.append(avg_sim)\n",
    "            \n",
    "\n",
    "    # Get top K images from ground truth (argsort returns indexes)\n",
    "    ground_truth_topk = np.argsort(ground_truth_similarity_scores)[::-1][:K]\n",
    "   \n",
    "    # Get image ids according to indexes\n",
    "    ground_truth_img_ids = [img_ids[i] for i in ground_truth_topk]\n",
    "    git_img_ids = [img_ids[i] for i in git_topk]\n",
    "\n",
    "    # Get image urls\n",
    "    ground_truth_img_urls = get_image_urls(ground_truth_img_ids, coco)\n",
    "    git_img_urls = get_image_urls(git_img_ids, coco)\n",
    "   \n",
    "    performance_dict = get_scores(git_img_ids, ground_truth_img_ids, performance_dict)\n",
    "    \n",
    "    return performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option avg_embed\n",
      "avg_embed object\n",
      "{'precision': 0.56, 'recall': 0.56, 'f1': 0.56, 'nDCG': 0.743}\n",
      "avg_embed action\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.54, 'recall': 0.54, 'f1': 0.54, 'nDCG': 0.828}\n",
      "avg_embed objects_with_count\n",
      "{'precision': 0.48, 'recall': 0.48, 'f1': 0.48, 'nDCG': 0.658}\n",
      "avg_embed reasoning\n",
      "{'precision': 0.52, 'recall': 0.52, 'f1': 0.52, 'nDCG': 0.745}\n",
      "avg_embed text_on_image\n",
      "{'precision': 0.62, 'recall': 0.62, 'f1': 0.62, 'nDCG': 0.746}\n",
      "Option highest_singular_score\n",
      "highest_singular_score object\n",
      "{'precision': 0.46, 'recall': 0.46, 'f1': 0.46, 'nDCG': 0.859}\n",
      "highest_singular_score action\n",
      "{'precision': 0.44, 'recall': 0.44, 'f1': 0.44, 'nDCG': 0.83}\n",
      "highest_singular_score objects_with_count\n",
      "{'precision': 0.4, 'recall': 0.4, 'f1': 0.4, 'nDCG': 0.702}\n",
      "highest_singular_score reasoning\n",
      "{'precision': 0.42, 'recall': 0.42, 'f1': 0.42, 'nDCG': 0.667}\n",
      "highest_singular_score text_on_image\n",
      "{'precision': 0.54, 'recall': 0.54, 'f1': 0.54, 'nDCG': 0.721}\n",
      "Option avg_score\n",
      "avg_score object\n",
      "{'precision': 0.54, 'recall': 0.54, 'f1': 0.54, 'nDCG': 0.549}\n",
      "avg_score action\n",
      "{'precision': 0.56, 'recall': 0.56, 'f1': 0.56, 'nDCG': 0.828}\n",
      "avg_score objects_with_count\n",
      "{'precision': 0.48, 'recall': 0.48, 'f1': 0.48, 'nDCG': 0.701}\n",
      "avg_score reasoning\n",
      "{'precision': 0.46, 'recall': 0.46, 'f1': 0.46, 'nDCG': 0.749}\n",
      "avg_score text_on_image\n",
      "{'precision': 0.62, 'recall': 0.62, 'f1': 0.62, 'nDCG': 0.738}\n"
     ]
    }
   ],
   "source": [
    "for option in options_with_all:\n",
    "    print(\"Option\", option)\n",
    "    for category in all_queries[\"categories\"].keys():\n",
    "        print(option, category)\n",
    "        query_group = all_queries[\"categories\"][category]\n",
    "        performance_dict = {\"precision\": [], \"recall\": [], \"f1\": [], \"nDCG\": []}\n",
    "        for query in query_group:\n",
    "            if option == \"avg_embed\":\n",
    "                \n",
    "                performance_dict = search(query=query, K=10, SIZE=500,performance_dict=performance_dict, avg_embed_sim=True)\n",
    "            elif option == \"highest_singular_score\":\n",
    "                performance_dict = search(query=query, K=10, SIZE=500,performance_dict=performance_dict,  highest_singular_sim=True)            \n",
    "            else: # avg_score    \n",
    "                performance_dict = search(query=query, K=10, SIZE=500,performance_dict=performance_dict,  highest_avg_sim=True)\n",
    "                          \n",
    "        avg_performance_dict = {k: sum(v) / len(v) for k, v in performance_dict.items()}\n",
    "        # round all values to 3 decimal places\n",
    "        avg_performance_dict = {k: round(v, 3) for k, v in avg_performance_dict.items()}\n",
    "        print(avg_performance_dict)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-search",
   "language": "python",
   "name": "image-search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
